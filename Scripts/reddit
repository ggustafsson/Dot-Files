#!/usr/bin/env ruby

# Description: This script automatically opens up all new posts found in
# specificed subreddits under new browser tabs. It keeps track of all the URLs
# opened at the last run.
#
# The subreddits that are supposed to be checked should be specified in the
# file ~/.reddit/config like this:
#
# aww
# funny
# pics
# wtf
#
# Usage: reddit
#        reddit <config>
#
# Written by GÃ¶ran Gustafsson (gustafsson.g at gmail.com).
# License: BSD 3-Clause.

require "fileutils"
require "io/console"
require "json"
require "open-uri"
require "shellwords"

$browser = "open -a Safari"
$directory = "#{Dir.home}/.reddit"
$filter_comments = 1 # Ignore URLs containing "/comments/".

# Check if the default config file should be used or not.
if ARGV.length == 0
  config_file = "#{$directory}/config"
else
  config_file = ARGV[0]
end

unless File.directory?($directory)
  puts "Directory '#{$directory}' does not exist! Creating it..."
  Dir.mkdir($directory)
end

unless File.exist?(config_file)
  abort "File '#{config_file}' does not exist! Exiting..."
end

subreddits = IO.readlines(config_file)
last_item = subreddits.count

if subreddits.empty?
  abort "No subreddits found in #{config_file}! Exiting..."
end

puts "Checking Reddit for new posts."

subreddits.each_with_index do |subreddit, index|
  subreddit = subreddit.chomp
  new_log_file = "#{$directory}/r_#{subreddit}_tmp"
  old_log_file = "#{$directory}/r_#{subreddit}"
  new_urls = []
  old_urls = []
  safe_urls = []

  puts "\nChecking r/#{subreddit} for new posts..."

  begin
    json_raw = open("https://reddit.com/r/#{subreddit}.json").read
    json_parsed = JSON.parse(json_raw)
  rescue
    abort "Failed when trying to fetch and parse json data! Exiting..."
  end

  if File.exist?(old_log_file)
    # Read old log file and put all visited URLs inside of array old_urls.
    File.readlines(old_log_file).each do |line|
      old_urls << Regexp.escape(line.strip)
    end
  end

  log_file = File.open(new_log_file, "w")

  # Loop through all posts on the front page of each subreddit.
  json_parsed["data"]["children"].each do |post|
    # Put the current posts URL into a variable.
    url = post["data"]["url"]

    # Write down every URL to the log file so it can be checked next run.
    log_file.puts url

    # Ignore all "URLs" that does not start with "http".
    next unless url.start_with?("http")
    # Ignore URLs containing "/comments/" if filtering is turned on.
    next if $filter_comments == 1 and url.include?("/comments/")

    # Save the URL in array if it has not been visited before.
    unless old_urls.include?(Regexp.escape(url))
      new_urls << url
    end
  end

  log_file.close
  FileUtils.mv(new_log_file, old_log_file) # Overwrite old log file.

  if new_urls.size >= 1
    new_urls.each { |url|
      puts "URL: #{url}"
      # Escape characters in URL to make them safe to use as command arguments.
      safe_urls << Shellwords.escape(url)
    }
    `#{$browser} #{safe_urls.join(" ")}` # Run browser with URLs as arguments.

    next if index == last_item - 1

    print "Press 'Return' key when ready to continue..."
    begin
      STDIN.noecho(&:gets)
    rescue Exception
      puts
      exit
    end
    puts
  else
    puts "No new posts found!"
  end
end
